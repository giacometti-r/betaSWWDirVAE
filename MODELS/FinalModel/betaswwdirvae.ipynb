{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as K\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.losses import Loss\nfrom tensorflow.keras.layers import Layer, Conv2D, Input, Flatten, Dense\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:16:48.569358Z","iopub.execute_input":"2023-11-18T17:16:48.569759Z","iopub.status.idle":"2023-11-18T17:16:48.575146Z","shell.execute_reply.started":"2023-11-18T17:16:48.569731Z","shell.execute_reply":"2023-11-18T17:16:48.574271Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"class MaxPoolingWithArgmax2D(Layer):\n    def __init__(\n            self,\n            pool_size=(2, 2),\n            strides=(2, 2),\n            padding='same',\n            **kwargs):\n        super(MaxPoolingWithArgmax2D, self).__init__(**kwargs)\n        self.padding = padding\n        self.pool_size = pool_size\n        self.strides = strides\n\n    def call(self, inputs, **kwargs):\n        padding = self.padding\n        pool_size = self.pool_size\n        strides = self.strides\n        ksize = [1, pool_size[0], pool_size[1], 1]\n        strides = [1, strides[0], strides[1], 1]\n        output, argmax = tf.nn.max_pool_with_argmax(inputs, ksize, strides, 'SAME')\n        argmax = tf.cast(argmax, tf.keras.backend.floatx())\n        return [output, argmax]\n\n    def compute_output_shape(self, input_shape):\n        ratio = (1, 2, 2, 1)\n        output_shape = [\n            dim // ratio[idx]\n            if dim is not None else None\n            for idx, dim in enumerate(input_shape)]\n        output_shape = tuple(output_shape)\n        return [output_shape, output_shape]\n\n    def compute_mask(self, inputs, mask=None):\n        return 2 * [None]\n","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:16:48.578589Z","iopub.execute_input":"2023-11-18T17:16:48.578868Z","iopub.status.idle":"2023-11-18T17:16:48.588818Z","shell.execute_reply.started":"2023-11-18T17:16:48.578843Z","shell.execute_reply":"2023-11-18T17:16:48.587971Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"class MaxUnpooling2D(Layer):\n    def __init__(self, up_size=(2, 2), **kwargs):\n        super(MaxUnpooling2D, self).__init__(**kwargs)\n        self.up_size = up_size\n    \n    def unpool(self, pool, ind, ksize=[1, 2, 2, 1]):\n        input_shape = tf.shape(pool)\n        output_shape = [input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3]]\n\n        flat_input_size = tf.math.reduce_prod(input_shape)\n        flat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n\n        pool_ = tf.reshape(pool, [flat_input_size])\n        batch_range = tf.reshape(tf.range(tf.cast(output_shape[0], tf.int64), dtype=ind.dtype),\n                                          shape=[input_shape[0], 1, 1, 1])\n        b = tf.ones_like(ind) * batch_range\n        b1 = tf.reshape(b, [flat_input_size, 1])\n        ind_ = tf.reshape(ind, [flat_input_size, 1])\n        ind_ = tf.concat([b1, ind_], 1)\n\n        ret = tf.scatter_nd(ind_, pool_, shape=tf.cast(flat_output_shape, tf.int64))\n        ret = tf.reshape(ret, output_shape)\n\n        set_input_shape = pool.get_shape()\n        set_output_shape = [set_input_shape[0], set_input_shape[1] * ksize[1], set_input_shape[2] * ksize[2], set_input_shape[3]]\n        ret.set_shape(set_output_shape)\n        return ret\n\n    def call(self, inputs, output_shape=None):\n        updates = inputs[0]\n        mask    = tf.cast(inputs[1], dtype=tf.int64)\n        ksize   = [1, self.up_size[0], self.up_size[1], 1]\n        return self.unpool(updates, mask, ksize)\n\n    def compute_output_shape(self, input_shape):\n        mask_shape = input_shape[1]\n        return (\n            mask_shape[0],\n            mask_shape[1] * self.up_size[0],\n            mask_shape[2] * self.up_size[1],\n            mask_shape[3]\n        )","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:16:48.590203Z","iopub.execute_input":"2023-11-18T17:16:48.590755Z","iopub.status.idle":"2023-11-18T17:16:48.605467Z","shell.execute_reply.started":"2023-11-18T17:16:48.590731Z","shell.execute_reply":"2023-11-18T17:16:48.604636Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"class Encoder(Model):\n    def __init__(self, latent_dim):\n        super(Encoder, self).__init__()\n        self.conv1 = Conv2D(16, (5, 5), activation='relu', padding='same')\n        self.conv2 = Conv2D(32, (3, 3), activation='relu', padding='same')\n        self.max_pool_argmax = MaxPoolingWithArgmax2D(pool_size=(7,7), strides=(7,7))\n        self.dense1 = Dense(latent_dim, activation='relu', bias_initializer='ones')\n        self.flatten = Flatten()\n        \n    def sample(self, alpha_hat):\n        shape = (alpha_hat.get_shape().as_list()[-2],alpha_hat.get_shape().as_list()[-1])\n        u = tf.random.uniform(shape=shape, minval=0, maxval=1)\n        v = tf.math.multiply(u,alpha_hat)\n        v = tf.math.multiply(v, tf.math.exp(tf.math.lgamma(alpha_hat)))\n        v = tf.math.pow(v, tf.math.divide(1.0,alpha_hat))\n        #no beta because vector of all ones\n        z = tf.math.divide(v,tf.math.reduce_sum(v)) #sum to one\n        return z\n    \n    def call(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x, mask_1 = self.max_pool_argmax(x)\n        L2M_encoder_1 = x\n        alpha_hat = self.flatten(x)\n        alpha_hat_shape = alpha_hat.shape\n        alpha_hat = self.dense1(alpha_hat)\n        z = self.sample(alpha_hat)\n        return z, mask_1, L2M_encoder_1, alpha_hat, alpha_hat_shape","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:16:48.606980Z","iopub.execute_input":"2023-11-18T17:16:48.607278Z","iopub.status.idle":"2023-11-18T17:16:48.619900Z","shell.execute_reply.started":"2023-11-18T17:16:48.607255Z","shell.execute_reply":"2023-11-18T17:16:48.619142Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"class Decoder(Model):\n    def __init__(self, original_shape):\n        super(Decoder, self).__init__()\n        self.unpool_1 = MaxUnpooling2D((7,7))\n        self.conv_1 = Conv2D(16, (3, 3), activation='relu', padding='same')\n        self.conv_2 = Conv2D(1, (5, 5), activation='sigmoid', padding='same')\n        \n\n    def call(self, x):\n        x[0] = Dense(x[2][1])(x[0])\n        x[0] = tf.reshape(x[0],x[3])\n        L2M_decoder_1 = x[0]\n        x = self.unpool_1([x[0],x[1]])\n        x = self.conv_1(x)\n        x = self.conv_2(x)\n        return x, L2M_decoder_1","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:16:48.620896Z","iopub.execute_input":"2023-11-18T17:16:48.621230Z","iopub.status.idle":"2023-11-18T17:16:48.633732Z","shell.execute_reply.started":"2023-11-18T17:16:48.621205Z","shell.execute_reply":"2023-11-18T17:16:48.632933Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"class betaSWWDirVAE(Model):\n    def __init__(self, input_dim):\n        super(betaSWWDirVAE, self).__init__()\n        self.encoder = Encoder(input_dim)\n        self.decoder = Decoder(input_dim)\n    \n    def compile(self, optimizer, loss):\n        super().compile(optimizer)\n        self.loss = loss\n\n    def call(self, x):\n        # Forward pass through the encoder and decoder\n        encoded, mask_1, L2M_encoder_1, alpha_hat, alpha_hat_shape = self.encoder(x)\n        decoded, L2M_decoder_1 = self.decoder([encoded, mask_1, alpha_hat_shape, L2M_encoder_1.shape])\n        return decoded, L2M_encoder_1, L2M_decoder_1, alpha_hat","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:16:48.635592Z","iopub.execute_input":"2023-11-18T17:16:48.635874Z","iopub.status.idle":"2023-11-18T17:16:48.644903Z","shell.execute_reply.started":"2023-11-18T17:16:48.635852Z","shell.execute_reply":"2023-11-18T17:16:48.644219Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def ELBO(mse, log_likelihood, y_true, y_pred, L2M_encoder_1, L2M_decoder_1, alpha_hat, alpha):\n    ll_loss = log_likelihood(y_true, y_pred)\n    \n    mse_loss = mse(y_true, y_pred)\n    \n    l2m_loss = tf.math.reduce_mean(tf.math.square(tf.math.subtract(L2M_encoder_1,L2M_decoder_1)))\n    \n    kld_loss = tf.math.subtract(tf.math.lgamma(alpha), tf.math.lgamma(alpha_hat))\n    kld_loss = tf.math.add(kld_loss, tf.math.multiply(tf.math.subtract(alpha_hat, alpha),tf.math.digamma(alpha_hat)))\n    kld_loss = tf.reduce_sum(kld_loss)\n    \n    total_loss = ll_loss + 0.2*l2m_loss + kld_loss\n    \n    return total_loss, mse_loss, ll_loss, l2m_loss, kld_loss","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:16:48.702507Z","iopub.execute_input":"2023-11-18T17:16:48.702778Z","iopub.status.idle":"2023-11-18T17:16:48.709412Z","shell.execute_reply.started":"2023-11-18T17:16:48.702755Z","shell.execute_reply":"2023-11-18T17:16:48.708560Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"def ClipIfNotNone(grad):\n            if grad is None:\n                return grad\n            return tf.clip_by_value(grad, -1, 1)","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:16:48.710897Z","iopub.execute_input":"2023-11-18T17:16:48.711203Z","iopub.status.idle":"2023-11-18T17:16:48.719190Z","shell.execute_reply.started":"2023-11-18T17:16:48.711178Z","shell.execute_reply":"2023-11-18T17:16:48.718208Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\nx_train = np.expand_dims(x_train.astype('float32') / 255., -1)\nx_test = np.expand_dims(x_test.astype('float32') / 255., -1)\n\nlatent_dim = 50\nmodel = betaSWWDirVAE(latent_dim)\n\noptimizer = tf.optimizers.Adam(learning_rate=1e-4)\nlog_likelihood = tf.keras.losses.BinaryCrossentropy()\nmse = tf.keras.losses.MeanSquaredError()\nalpha = (1 - 1/latent_dim) * tf.ones(shape=(latent_dim,))\n\nbatch_size = 128\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train))\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test))\ntest_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size)\n\nepochs = 10\nfor epoch in range(epochs):\n    print('___________________________')\n    print(f'_____EPOCH_{epoch}________')\n    print('___________________________')\n\n    # Iterate over the batches of the dataset.\n    for step, (x_batch_train) in enumerate(train_dataset):\n        # Open a GradientTape to record the operations run\n        # during the forward pass, which enables auto-differentiation.\n        with tf.GradientTape() as tape:\n            prediction, L2M_encoder_1, L2M_decoder_1, alpha_hat = model(x_batch_train, training=True)\n            # Compute the loss value for this minibatch.\n            total_loss, mse_loss, ll_loss, l2m_loss, kld_loss = ELBO(mse, log_likelihood, x_batch_train, prediction, L2M_encoder_1, L2M_decoder_1, alpha_hat, alpha)\n\n        # Use the gradient tape to automatically retrieve\n        # the gradients of the trainable variables with respect to the loss.\n        grads = tape.gradient(total_loss, model.trainable_weights)\n        grads = [ClipIfNotNone(grad) for grad in grads]\n        # Run one step of gradient descent by updating\n        # the value of the variables to minimize the loss.\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n        # Log every 250 batches.\n        if step % 250 == 0:\n            print(f\"Training loss at step {step}: {float(total_loss):.4f}\")\n            print(f\"mse loss at step {step}: {float(mse_loss):.4f}\")\n            print(f\"LL loss at step {step}: {float(ll_loss):.4f}\")\n            print(f\"kld loss at step {step}: {float(kld_loss):.4f}\")\n            print(f\"l2m loss at step {step}: {float(l2m_loss):.4f}\\n\")\n            \n    val_loss = []\n    for step, (x_batch_test) in enumerate(test_dataset):\n        val_prediction, val_L2M_encoder_1, val_L2M_decoder_1, val_alpha_hat = model(x_batch_test, training=False)\n        val_total_loss, vale_mse_loss, val_ll_loss, val_l2m_loss, val_kld_loss = ELBO(mse, log_likelihood, x_batch_test, val_prediction, val_L2M_encoder_1, val_L2M_decoder_1, val_alpha_hat, alpha)\n        val_loss.append(val_total_loss.numpy())\n    val_loss = np.mean(np.array(val_loss))\n    print('AVERAGE VALIDATION LOSS:', val_loss)\n    print()\n    image, img_L2M_encoder_1, img_L2M_decoder_1, img_alpha_hat = model(x_test[0].reshape((1,28,28,1)), training=False)\n    print('ORIGINAL')\n    plt.imshow(x_test[0].reshape((28,28)))\n    plt.show()\n    print()\n    print('RECONSTRUCTION')\n    plt.imshow(tf.reshape(image, (28,28)).numpy())\n    plt.show()\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:16:48.720870Z","iopub.execute_input":"2023-11-18T17:16:48.721312Z","iopub.status.idle":"2023-11-18T17:17:36.644720Z","shell.execute_reply.started":"2023-11-18T17:16:48.721287Z","shell.execute_reply":"2023-11-18T17:17:36.642936Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"___________________________\n_____EPOCH_0________\n___________________________\nTraining loss at step 0: 105.2372\nmse loss at step 0: 0.2317\nLL loss at step 0: 0.6931\nkld loss at step 0: 104.5419\nl2m loss at step 0: 0.0107\n\nTraining loss at step 250: 1.2425\nmse loss at step 250: 0.1794\nLL loss at step 250: 0.5886\nkld loss at step 250: 0.6538\nl2m loss at step 250: 0.0003\n\nAVERAGE VALIDATION LOSS: 0.546981\n\nORIGINAL\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"\nRECONSTRUCTION\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/OQEPoAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAds0lEQVR4nO3dfXCU9d3v8c8mkgUkhMaYpxIw4AOtQHqXSppRKZYcQjrjiHL39ukPcBwYbXCKqdVJR0Vtz0mLM9bRSfGfFuqM+DQjcPT00NFowtgGekA5DKdtbpJJSxhIqMyQQJBAs7/zB8ftWQma/bF7fa/dvF8z1wzZva5c3/3lSj4su/kQcc45AQAQsBzrAQAA4xMBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABOXWQ/webFYTEeOHFF+fr4ikYj1OACAJDnndPLkSZWXlysn5+LPc0IXQEeOHFFFRYX1GACAS9Tb26vp06df9P7QBVB+fr4k6Y0/XqXJU8b+L4TPH/4vSZ/rP/cHGHQhfjLn28UU2EPyGdBzuKB6qXzG81qG0H9xPQR4PWSjCWVDSR9Te9V/JrX/2aFz2lT/3+M/zy8mbQHU0tKiZ599Vn19faqqqtKLL76ohQsXfulxn/2z2+QpObo8f+wBNOHyvKRnzJk4MeljvIX4G8B5zub9wy1ZQQZQQF8nn7XzmS0SS/6Y8wd6HhcEAuiS5E4eSfqYvCkTvM71ZS+jpOVNCK+//roaGxu1fv16ffTRR6qqqlJdXZ2OHTuWjtMBADJQWgLoueee0+rVq3Xffffp61//ul566SVNnjxZv/nNb9JxOgBABkp5AJ09e1Z79+5VbW3tv06Sk6Pa2lp1dHRcsP/w8LAGBwcTNgBA9kt5AH3yyScaGRlRSUlJwu0lJSXq6+u7YP/m5mYVFBTEN94BBwDjg/kvojY1NWlgYCC+9fb2Wo8EAAhAyt8FV1RUpNzcXPX39yfc3t/fr9LS0gv2j0ajikajqR4DABByKX8GlJeXpwULFqi1tTV+WywWU2trq2pqalJ9OgBAhkrL7wE1NjZq5cqV+ta3vqWFCxfq+eef19DQkO677750nA4AkIHSEkB33nmn/vGPf+jJJ59UX1+fvvGNb2jHjh0XvDEBADB+RZxzQf0++5gMDg6qoKBA97f9R1K/ffs///BvSZ9r+nu+vyYeXpEAv5zOoyzWZz6f8yBDBNVJ5Ckbr9cTVyf/vOPkN88ktX/s0zPqXfOMBgYGNHXq1IvuZ/4uOADA+EQAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMBEWtqwU6Ht0GzlTp445v0L9ydfADjxf/yvpI/xFvHIeudRlhrUeUIukpsb2LlcLFR9vol8v7Y+11E2Cup7I8D1LqmZl/Qx/5w8Oan9R4bHth9XGQDABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARGjbsAMR9sbfoObzPE8kJ/kG8qCao0PdUK0A1y7s13hQgmx8Z83HjJUCAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgYnyXkeKShL3wM8xYu4BREBpKfFUAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYCG0ZqXPnt7Ef4HOSmMdBCq7Y0Gc+ShcvSSQnkvQxPsWiYT6P77k8TxTMeYLk8z04Tr/XM/8RAAAyEgEEADCR8gB66qmnFIlEErY5c+ak+jQAgAyXlteArr/+er333nv/OslloX2pCQBgJC3JcNlll6m0tDQdnxoAkCXS8hrQwYMHVV5erlmzZunee+/VoUOHLrrv8PCwBgcHEzYAQPZLeQBVV1dr8+bN2rFjhzZu3Kienh7dfPPNOnny5Kj7Nzc3q6CgIL5VVFSkeiQAQAilPIDq6+v1/e9/X/Pnz1ddXZ1+97vf6cSJE3rjjTdG3b+pqUkDAwPxrbe3N9UjAQBCKO3vDpg2bZquvfZadXV1jXp/NBpVNBpN9xgAgJBJ++8BnTp1St3d3SorK0v3qQAAGSTlAfTII4+ovb1df/vb3/THP/5Rt99+u3Jzc3X33Xen+lQAgAyW8n+CO3z4sO6++24dP35cV155pW666Sbt2rVLV155ZapPBQDIYCkPoNdeey3VnzJ8gioODHPpqRRY6WIkN9fjNAGVaQbIt1g0qHNl45ojveiCAwCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYCLt/yFdqDnP8sSc5Msxg+JXIun39xC/cszwrp0vr3UIqmjWkxsZSfoYr2sv+dOEn2+57zgU7u8CAEDWIoAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYGN9t2GHn0arr1Wzt3d6bfLO1iyXfQB7J9WnQDnnNclCNyZ6t236t6h7t8j7zBdk27dOYH/FpiR+feAYEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARGjLSHNynHJyPIoAkxH20kDPIsmgzuNVPulznpHgikWDekxBlX1GcvyKO4NaBy9BfV9Iks+PiCDLUjMcz4AAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYCG0ZqXMROZdEE6BPaWCQpYYefAorw86vUDOY4s4gBVbk6nmeMK95kLP5nSvcP1fChJUCAJgggAAAJpIOoJ07d+rWW29VeXm5IpGItm3blnC/c05PPvmkysrKNGnSJNXW1urgwYOpmhcAkCWSDqChoSFVVVWppaVl1Ps3bNigF154QS+99JJ2796tyy+/XHV1dTpz5swlDwsAyB5Jvwmhvr5e9fX1o97nnNPzzz+vxx9/XLfddpsk6eWXX1ZJSYm2bdumu+6669KmBQBkjZS+BtTT06O+vj7V1tbGbysoKFB1dbU6OjpGPWZ4eFiDg4MJGwAg+6U0gPr6+iRJJSUlCbeXlJTE7/u85uZmFRQUxLeKiopUjgQACCnzd8E1NTVpYGAgvvX29lqPBAAIQEoDqLS0VJLU39+fcHt/f3/8vs+LRqOaOnVqwgYAyH4pDaDKykqVlpaqtbU1ftvg4KB2796tmpqaVJ4KAJDhkn4X3KlTp9TV1RX/uKenR/v27VNhYaFmzJihdevW6Wc/+5muueYaVVZW6oknnlB5ebmWL1+eyrkBABku6QDas2ePbrnllvjHjY2NkqSVK1dq8+bNevTRRzU0NKQ1a9boxIkTuummm7Rjxw5NnDgxdVMDADJe0gG0ePFiOXfxYr9IJKJnnnlGzzzzzCUNFok4RSJjLxBMprc0U2RlcaeLeRwS4HtlPObzKrUN6jyeQl0sOjKShkkudq7kj4nk5iZ/npCX56aL+bvgAADjEwEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARNJt2EGJRJRUG7YCbMP2afD14dOQG/Zm6yAbnb0ENZ/HeYJsOg/qGs9G47XZ2kfIfxoAALIVAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAE6EtI3VOci6JQkSP/r8gCxcpKPx/Ql5gGuYSziCvIZ9zBbV2kdzcQM4jSW5kJPmDYh7H5AT3mMKEZ0AAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMhLaMNBC+JZcehZo+RY1hLoSUJOfRuei15j7rHWBhZVB8HpNXmaaCu45CX9Lrdb16rHnIS3rTJfMfAQAgIxFAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADAR2jLSSESKRMZeVJjErnFBFjUGVbrodR6fIkRfAZUu+q53mL+2kZwAv04eQl8sGpSIR5FrFhSL+hifjxoAYI4AAgCYSDqAdu7cqVtvvVXl5eWKRCLatm1bwv2rVq1SJBJJ2JYtW5aqeQEAWSLpABoaGlJVVZVaWlouus+yZct09OjR+Pbqq69e0pAAgOyT9JsQ6uvrVV9f/4X7RKNRlZaWeg8FAMh+aXkNqK2tTcXFxbruuuv04IMP6vjx4xfdd3h4WIODgwkbACD7pTyAli1bppdfflmtra36xS9+ofb2dtXX12vkIm95bm5uVkFBQXyrqKhI9UgAgBBK+e8B3XXXXfE/z5s3T/Pnz9fs2bPV1tamJUuWXLB/U1OTGhsb4x8PDg4SQgAwDqT9bdizZs1SUVGRurq6Rr0/Go1q6tSpCRsAIPulPYAOHz6s48ePq6ysLN2nAgBkkKT/Ce7UqVMJz2Z6enq0b98+FRYWqrCwUE8//bRWrFih0tJSdXd369FHH9XVV1+turq6lA4OAMhsSQfQnj17dMstt8Q//uz1m5UrV2rjxo3av3+/fvvb3+rEiRMqLy/X0qVL9dOf/lTRaDR1UwMAMl7SAbR48WI5d/HSwd///veXNNBnnJOc8yj1C0BghZ/ZWFAY8sfk87UNc4Gpr+AKVsP5PX4pXCzc13iYsFIAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMp/y+5rYS0OPtfwtwCHebZMkBQDdrZKPTN8h7niuTmepwm3O3o6cJPHgCACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACaypow0G/kUVo7XUkNTHoWVLsbf/QC+CwAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJjImjLSSMg7OIMqFvU5jy83MpL8QZGQ/53Ho1g0sPP4rF1Qj0dSJDc3oBNNSP4Yz3XwKY3Nyu+LNBmfjxoAYI4AAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJ0JaRxmIRRWJjL9aMBNe5KMWSLxt0Sr6oMahiUZ/SU0nBFSgGVdyp4Ao1vdbcZx2c79c2+8pzAzNOi0V9sFIAABMEEADARFIB1NzcrBtuuEH5+fkqLi7W8uXL1dnZmbDPmTNn1NDQoCuuuEJTpkzRihUr1N/fn9KhAQCZL6kAam9vV0NDg3bt2qV3331X586d09KlSzU0NBTf5+GHH9bbb7+tN998U+3t7Tpy5IjuuOOOlA8OAMhsSb0JYceOHQkfb968WcXFxdq7d68WLVqkgYEB/frXv9aWLVv03e9+V5K0adMmfe1rX9OuXbv07W9/O3WTAwAy2iW9BjQwMCBJKiwslCTt3btX586dU21tbXyfOXPmaMaMGero6Bj1cwwPD2twcDBhAwBkP+8AisViWrdunW688UbNnTtXktTX16e8vDxNmzYtYd+SkhL19fWN+nmam5tVUFAQ3yoqKnxHAgBkEO8Aamho0IEDB/Taa69d0gBNTU0aGBiIb729vZf0+QAAmcHrF1HXrl2rd955Rzt37tT06dPjt5eWlurs2bM6ceJEwrOg/v5+lZaWjvq5otGootGozxgAgAyW1DMg55zWrl2rrVu36v3331dlZWXC/QsWLNCECRPU2toav62zs1OHDh1STU1NaiYGAGSFpJ4BNTQ0aMuWLdq+fbvy8/Pjr+sUFBRo0qRJKigo0P3336/GxkYVFhZq6tSpeuihh1RTU8M74AAACZIKoI0bN0qSFi9enHD7pk2btGrVKknSL3/5S+Xk5GjFihUaHh5WXV2dfvWrX6VkWABA9kgqgNwYig0nTpyolpYWtbS0eA8lne9CTKYPMRZMh+R5HkWNYS5d9C7g9CjH9Crh9Ch3DHS9vconky+09TuPZ0tvQIWafkW4HmsXIJ9rz7sQOMPRBQcAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMOH1P6IGIRJxikTG3hCbxK4mfNpuvRqdPVqM3Yhfu3BgjdMerdtSkPXowfBpLXe+xdE+ax5Qg7ZXe7vXNeRnvDZb++AZEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABOhLSN1LiLnkii79On/C7Cg0IdfqaFv+2TygipYdbFw/z3Jp8zVr1g0uK9tUMWiXjy+b30LQkNduBvmr9EYZf4jAABkJAIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZCW0aaLOcTpVlQ5ofRBVk+6VVY6VE+6Vfk6rcOYS7qDbKcNrAC2HH6s2h8PmoAgDkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmsqaMNOLZuYhgeZdjBiTM83mVnvqfLLhzhRnrkFasLgDABAEEADCRVAA1NzfrhhtuUH5+voqLi7V8+XJ1dnYm7LN48WJFIpGE7YEHHkjp0ACAzJdUALW3t6uhoUG7du3Su+++q3Pnzmnp0qUaGhpK2G/16tU6evRofNuwYUNKhwYAZL6k3oSwY8eOhI83b96s4uJi7d27V4sWLYrfPnnyZJWWlqZmQgBAVrqk14AGBgYkSYWFhQm3v/LKKyoqKtLcuXPV1NSk06dPX/RzDA8Pa3BwMGEDAGQ/77dhx2IxrVu3TjfeeKPmzp0bv/2ee+7RzJkzVV5erv379+uxxx5TZ2en3nrrrVE/T3Nzs55++mnfMQAAGco7gBoaGnTgwAF9+OGHCbevWbMm/ud58+aprKxMS5YsUXd3t2bPnn3B52lqalJjY2P848HBQVVUVPiOBQDIEF4BtHbtWr3zzjvauXOnpk+f/oX7VldXS5K6urpGDaBoNKpoNOozBgAggyUVQM45PfTQQ9q6dava2tpUWVn5pcfs27dPklRWVuY1IAAgOyUVQA0NDdqyZYu2b9+u/Px89fX1SZIKCgo0adIkdXd3a8uWLfre976nK664Qvv379fDDz+sRYsWaf78+Wl5AACAzJRUAG3cuFHS+V82/f9t2rRJq1atUl5ent577z09//zzGhoaUkVFhVasWKHHH388ZQMDALJD0v8E90UqKirU3t5+SQMBAMaH0LZhL73qr4pOmTDm/bdWLUz6HPm1/5b0MZLkPEqJaevGpeK6QyqcmDX2n6ufOT3jn0ntH/t0bPtTRgoAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMBEaMtI/1vp/9bU/LHn45TFw0mf4+Wp3076GADIZOVl/Ukf8+/l/yep/c+cOqf/Oob9eAYEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABOh64JzzkmSBk/Fkjpu+NS5pM8V+/RM0scAQCb751DyvZlnkvz5Ojz0T0n/+nl+MRH3ZXsE7PDhw6qoqLAeAwBwiXp7ezV9+vSL3h+6AIrFYjpy5Ijy8/MViUQS7hscHFRFRYV6e3s1depUowntsQ7nsQ7nsQ7nsQ7nhWEdnHM6efKkysvLlZNz8Vd6QvdPcDk5OV+YmJI0derUcX2BfYZ1OI91OI91OI91OM96HQoKCr50H96EAAAwQQABAExkVABFo1GtX79e0WjUehRTrMN5rMN5rMN5rMN5mbQOoXsTAgBgfMioZ0AAgOxBAAEATBBAAAATBBAAwETGBFBLS4uuuuoqTZw4UdXV1frTn/5kPVLgnnrqKUUikYRtzpw51mOl3c6dO3XrrbeqvLxckUhE27ZtS7jfOacnn3xSZWVlmjRpkmpra3Xw4EGbYdPoy9Zh1apVF1wfy5Ytsxk2TZqbm3XDDTcoPz9fxcXFWr58uTo7OxP2OXPmjBoaGnTFFVdoypQpWrFihfr7+40mTo+xrMPixYsvuB4eeOABo4lHlxEB9Prrr6uxsVHr16/XRx99pKqqKtXV1enYsWPWowXu+uuv19GjR+Pbhx9+aD1S2g0NDamqqkotLS2j3r9hwwa98MILeumll7R7925dfvnlqqur05kz2VU2+2XrIEnLli1LuD5effXVACdMv/b2djU0NGjXrl169913de7cOS1dulRDQ0PxfR5++GG9/fbbevPNN9Xe3q4jR47ojjvuMJw69cayDpK0evXqhOthw4YNRhNfhMsACxcudA0NDfGPR0ZGXHl5uWtubjacKnjr1693VVVV1mOYkuS2bt0a/zgWi7nS0lL37LPPxm87ceKEi0aj7tVXXzWYMBifXwfnnFu5cqW77bbbTOaxcuzYMSfJtbe3O+fOf+0nTJjg3nzzzfg+f/nLX5wk19HRYTVm2n1+HZxz7jvf+Y774Q9/aDfUGIT+GdDZs2e1d+9e1dbWxm/LyclRbW2tOjo6DCezcfDgQZWXl2vWrFm69957dejQIeuRTPX09Kivry/h+igoKFB1dfW4vD7a2tpUXFys6667Tg8++KCOHz9uPVJaDQwMSJIKCwslSXv37tW5c+cSroc5c+ZoxowZWX09fH4dPvPKK6+oqKhIc+fOVVNTk06fPm0x3kWFroz08z755BONjIyopKQk4faSkhL99a9/NZrKRnV1tTZv3qzrrrtOR48e1dNPP62bb75ZBw4cUH5+vvV4Jvr6+iRp1Ovjs/vGi2XLlumOO+5QZWWluru79ZOf/ET19fXq6OhQbm6u9XgpF4vFtG7dOt14442aO3eupPPXQ15enqZNm5awbzZfD6OtgyTdc889mjlzpsrLy7V//3499thj6uzs1FtvvWU4baLQBxD+pb6+Pv7n+fPnq7q6WjNnztQbb7yh+++/33AyhMFdd90V//O8efM0f/58zZ49W21tbVqyZInhZOnR0NCgAwcOjIvXQb/IxdZhzZo18T/PmzdPZWVlWrJkibq7uzV79uygxxxV6P8JrqioSLm5uRe8i6W/v1+lpaVGU4XDtGnTdO2116qrq8t6FDOfXQNcHxeaNWuWioqKsvL6WLt2rd555x198MEHCf99S2lpqc6ePasTJ04k7J+t18PF1mE01dXVkhSq6yH0AZSXl6cFCxaotbU1flssFlNra6tqamoMJ7N36tQpdXd3q6yszHoUM5WVlSotLU24PgYHB7V79+5xf30cPnxYx48fz6rrwzmntWvXauvWrXr//fdVWVmZcP+CBQs0YcKEhOuhs7NThw4dyqrr4cvWYTT79u2TpHBdD9bvghiL1157zUWjUbd582b35z//2a1Zs8ZNmzbN9fX1WY8WqB/96Eeura3N9fT0uD/84Q+utrbWFRUVuWPHjlmPllYnT550H3/8sfv444+dJPfcc8+5jz/+2P397393zjn385//3E2bNs1t377d7d+/3912222usrLSffrpp8aTp9YXrcPJkyfdI4884jo6OlxPT49777333De/+U13zTXXuDNnzliPnjIPPvigKygocG1tbe7o0aPx7fTp0/F9HnjgATdjxgz3/vvvuz179riamhpXU1NjOHXqfdk6dHV1uWeeecbt2bPH9fT0uO3bt7tZs2a5RYsWGU+eKCMCyDnnXnzxRTdjxgyXl5fnFi5c6Hbt2mU9UuDuvPNOV1ZW5vLy8txXv/pVd+edd7quri7rsdLugw8+cJIu2FauXOmcO/9W7CeeeMKVlJS4aDTqlixZ4jo7O22HToMvWofTp0+7pUuXuiuvvNJNmDDBzZw5061evTrr/pI22uOX5DZt2hTf59NPP3U/+MEP3Fe+8hU3efJkd/vtt7ujR4/aDZ0GX7YOhw4dcosWLXKFhYUuGo26q6++2v34xz92AwMDtoN/Dv8dAwDAROhfAwIAZCcCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAm/i8vSakeh3UvYwAAAABJRU5ErkJggg=="},"metadata":{}},{"name":"stdout","text":"\n___________________________\n_____EPOCH_1________\n___________________________\nTraining loss at step 0: 0.5416\nmse loss at step 0: 0.1211\nLL loss at step 0: 0.4612\nkld loss at step 0: 0.0804\nl2m loss at step 0: 0.0000\n\nTraining loss at step 250: 0.4318\nmse loss at step 250: 0.1060\nLL loss at step 250: 0.4224\nkld loss at step 250: 0.0095\nl2m loss at step 250: 0.0000\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[63], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m grads \u001b[38;5;241m=\u001b[39m [ClipIfNotNone(grad) \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Run one step of gradient descent by updating\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# the value of the variables to minimize the loss.\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Log every 250 batches.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m250\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1230\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m   1229\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[0;32m-> 1230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    651\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m--> 652\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1260\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mesh \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_with_dtensor:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;66;03m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m-> 1260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply_gradients_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribute_lib\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1352\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m-> 1352\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m   1357\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:2992\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2989\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   2990\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2991\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2994\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   2995\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4062\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   4060\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   4061\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 4062\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4068\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4064\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   4065\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   4066\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   4067\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 4068\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   4070\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py:1347\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad):\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit_compile:\n\u001b[0;32m-> 1347\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_step_xla\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1349\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:864\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:17:36.645676Z","iopub.status.idle":"2023-11-18T17:17:36.646152Z","shell.execute_reply.started":"2023-11-18T17:17:36.645904Z","shell.execute_reply":"2023-11-18T17:17:36.645925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_recon = model.predict(x_test[:25])\nx_plot = np.concatenate((x_test[:25], x_recon[0]), axis=1)\nprint(x_plot.shape)\nx_plot = x_plot.reshape((28,1,10,56))\nx_plot = np.vstack([np.hstack(x) for x in x_plot])\nplt.figure()\nplt.axis('off')\nplt.title('Test Samples: Originals/Reconstructions')\nplt.imshow(x_plot, interpolation='none', cmap='gray')\nplt.savefig('reconstructions.png')","metadata":{"execution":{"iopub.status.busy":"2023-11-18T17:17:36.647467Z","iopub.status.idle":"2023-11-18T17:17:36.647915Z","shell.execute_reply.started":"2023-11-18T17:17:36.647713Z","shell.execute_reply":"2023-11-18T17:17:36.647740Z"},"trusted":true},"execution_count":null,"outputs":[]}]}