{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.utils.data\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.cuda.amp import autocast, GradScaler\ntorch.manual_seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-20T00:08:36.938405Z","iopub.execute_input":"2023-11-20T00:08:36.938767Z","iopub.status.idle":"2023-11-20T00:08:36.947096Z","shell.execute_reply.started":"2023-11-20T00:08:36.938738Z","shell.execute_reply":"2023-11-20T00:08:36.946162Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7beec544a2b0>"},"metadata":{}}]},{"cell_type":"code","source":"torch.__version__","metadata":{"execution":{"iopub.status.busy":"2023-11-20T00:10:14.338163Z","iopub.execute_input":"2023-11-20T00:10:14.338792Z","iopub.status.idle":"2023-11-20T00:10:14.344649Z","shell.execute_reply.started":"2023-11-20T00:10:14.338759Z","shell.execute_reply":"2023-11-20T00:10:14.343733Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'2.0.0'"},"metadata":{}}]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, latent_dim):\n        super(Encoder, self).__init__()\n        # Define the architecture of the encoder\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding='same')\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding='same')\n        self.maxpool1 = nn.MaxPool2d(kernel_size=7, stride=7, padding='same', return_indices=True)\n        self.flatten = nn.Flatten()\n        self.softplus = nn.Softplus()\n        \n    def sample(self, alpha_hat):\n        u = torch.rand(size=alpha_hat.size(), requires_grad=True).to(device)\n        v = torch.pow(u * alpha_hat * torch.exp(torch.lgamma(alpha_hat)),1.0/alpha_hat)\n        z = v / torch.sum(v)\n        return z\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x, mask_1 = self.maxpool1(x)\n        x = l2m_pool_1\n        x_size = x.size()\n        alpha_hat = self.flatten(x)\n        alpha_hat_size = alpha_hat.size()\n        alpha_hat = self.softplus(nn.Linear(alpha_hat.size()[1],latent_dim)(alpha_hat))\n        z = self.sample(alpha_hat)\n        return z, mask_1, l2m_pool_1, x_size, alpha_hat, alpha_hat_size\n","metadata":{"execution":{"iopub.status.busy":"2023-11-20T00:08:36.957777Z","iopub.execute_input":"2023-11-20T00:08:36.958087Z","iopub.status.idle":"2023-11-20T00:08:36.968137Z","shell.execute_reply.started":"2023-11-20T00:08:36.958063Z","shell.execute_reply":"2023-11-20T00:08:36.967213Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n    def __init__(self, latent_dim):\n        super(Decoder, self).__init__()\n        # Define the architecture of the decoder\n        self.maxunpool = nn.MaxUnpool2D(kernel_size=7,padding='same')\n        self.conv1 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding='same')\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2D(in_channels=16, out_channels=1, kernel_size=5, stride=1, padding='same')\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, inputs):\n        z, mask_1, x_size, alpha_hat_size = inputs\n        x_hat = nn.Linear(latent_dim, alpha_hat_size[1])(z)\n        x_hat = torch.reshape(x_hat, x_size)\n        x_hat = l2m_unpool_1\n        x_hat = self.maxunpool(x, mask_1)\n        x_hat = self.relu(self.conv1(x_hat))\n        x_hat = self.sigmoid(self.conv2(x_hat))\n        return x_hat, l2m_unpool_1","metadata":{"execution":{"iopub.status.busy":"2023-11-20T00:08:36.969533Z","iopub.execute_input":"2023-11-20T00:08:36.969809Z","iopub.status.idle":"2023-11-20T00:08:36.981861Z","shell.execute_reply.started":"2023-11-20T00:08:36.969777Z","shell.execute_reply":"2023-11-20T00:08:36.980995Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class SWWAE(nn.Module):\n    def __init__(self, latent_dim):\n        super(SWWAE, self).__init__()\n        self.encoder = Encoder(latent_dim)\n        self.decoder = Decoder(latent_dim)\n\n    def forward(self, x):\n        z, mask_1, l2m_pool_1, x_size, alpha_hat, alpha_hat_size = self.encoder(x)\n        x_hat, l2m_unpool_1 = self.decoder([z, mask_1, x_size, alpha_hat_size])\n        return x_hat, l2m_pool_1, l2m_unpool_1, alpha_hat","metadata":{"execution":{"iopub.status.busy":"2023-11-20T00:08:36.983414Z","iopub.execute_input":"2023-11-20T00:08:36.983766Z","iopub.status.idle":"2023-11-20T00:08:36.994660Z","shell.execute_reply.started":"2023-11-20T00:08:36.983706Z","shell.execute_reply":"2023-11-20T00:08:36.993805Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def loss_fn(x, x_hat, l2m_pool_1, l2m_unpool_1, alpha_hat, alpha):\n    ll_loss = F.binary_cross_entropy_with_logits(x_hat.view(-1, 28*28), x.view(-1, 28*28), reduction='sum')\n    \n    l2m_loss = F.mse_loss(l2m_pool_1, l2m_unpool_1)\n    \n    lgamma_alpha = torch.lgamma(alpha).to(device)\n    lgamma_alpha_hat = torch.lgamma(alpha_hat).to(device)\n    digamma_alpha_hat = torch.digamma(alpha_hat).to(device)\n    \n    kld = torch.sum(lgamma_alpha - lgamma_alpha_hat + (alpha_hat - alpha) * digamma_alpha_hat)\n    \n    total_loss = ll_loss + l2m_loss + kld","metadata":{"execution":{"iopub.status.busy":"2023-11-20T00:08:36.996152Z","iopub.execute_input":"2023-11-20T00:08:36.996417Z","iopub.status.idle":"2023-11-20T00:08:37.004014Z","shell.execute_reply.started":"2023-11-20T00:08:36.996394Z","shell.execute_reply":"2023-11-20T00:08:37.003185Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def update_alpha_mme(z):\n    dirichlet = torch.distributions.Dirichlet(z)\n    p_set = dirichlet.sample()\n    N, K = p_set.size()\n\n    mu1_tilde = torch.mean(p_set, axis=0)\n    mu2_tilde = torch.mean(torch.pow(p_set,2), axis=0)\n\n    S = 1/K * torch.sum((mu1_tilde-mu2_tilde) / (mu2_tilde-torch.pow(mu1_tilde,2)))\n\n    alpha = S/N * torch.sum(p_set, axis=0)\n    \n    return alpha","metadata":{"execution":{"iopub.status.busy":"2023-11-20T00:08:37.005597Z","iopub.execute_input":"2023-11-20T00:08:37.006347Z","iopub.status.idle":"2023-11-20T00:08:37.014571Z","shell.execute_reply.started":"2023-11-20T00:08:37.006314Z","shell.execute_reply":"2023-11-20T00:08:37.013749Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor()),\n                                           batch_size=100, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n                                          batch_size=100, shuffle=True)\ncuda = torch.cuda.is_available()\n\ndevice = torch.device(\"cuda\" if cuda else \"cpu\")\n\nlatent_dim = 50\n\nmodel = SWWAE(latent_dim).to(device)\n\nparams = model.parameters()\noptimizer = optim.Adam(params, lr=5e-4)\n\nalpha =  ((1 - 1/latent_dim) * torch.ones(size=(latent_dim,))).to(device)\n\nepochs = 300\n\nscaler = GradScaler()\n\nfor epoch in range(epochs):\n    model.train()\n    for batch_idx, (x, _) in enumerate(train_loader): \n        x = x.to(device)\n        optimizer.zero_grad()\n        with autocast():\n            x_hat, l2m_pool_1, l2m_unpool_1, alpha_hat = model(x)\n            loss = loss_fn(x, x_hat, l2m_pool_1, l2m_unpool_1, alpha_hat, alpha)\n        scaler.scale(loss).backward()\n        #torch.nn.utils.clip_grad_norm_(params, 1.0)\n        scaler.step(optimizer)\n        scaler.update()\n    print(f'loss at end of epoch {epoch}: {loss.item()}')\n    \n    model.eval()\n    with torch.no_grad():\n        for i, (val_x, _) in enumerate(test_loader):\n            val_x = val_x.to(device)\n            val_x_hat, val_l2m_pool_1, val_l2m_unpool_1, val_alpha_hat = model(val_x)\n            test_loss = loss_fn(val_x, val_x_hat, val_l2m_pool_1, val_l2m_unpool_1, val_alpha_hat, alpha)\n    print(f'test loss at end of epoch {epoch}: {test_loss.item()}')\n    \n    if epoch == 0:\n        print('ORIGINAL')\n        plt.imshow(test_loader.dataset[0][0].numpy().reshape(28,28))\n        plt.show()\n    with torch.no_grad():\n        sample = test_loader.dataset[0][0].to(device)\n        img, img_l2m_pool_1, img_l2m_unpool_1, img_alpha_hat = model(sample)\n    img = torch.sigmoid(img)\n    img = img.to('cpu').numpy().reshape(28,28)\n    print('RECONSTRUCTED')\n    plt.imshow(img)\n    plt.show()\n    \n    if epoch % 50 == 0 and epoch >= 200 and epoch < 299:\n        alpha = update_alpha_mme(z)\n        print('alpha:', alpha)","metadata":{"execution":{"iopub.status.busy":"2023-11-20T00:08:37.071429Z","iopub.execute_input":"2023-11-20T00:08:37.071703Z","iopub.status.idle":"2023-11-20T00:08:37.254717Z","shell.execute_reply.started":"2023-11-20T00:08:37.071679Z","shell.execute_reply":"2023-11-20T00:08:37.253555Z"},"trusted":true},"execution_count":14,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cuda \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSWWAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mparameters()\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(params, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)\n","Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mSWWAE.__init__\u001b[0;34m(self, latent_dim)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(SWWAE, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m Encoder(latent_dim)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36mDecoder.__init__\u001b[0;34m(self, latent_dim)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(Decoder, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define the architecture of the decoder\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxunpool \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMaxUnpool2D\u001b[49m(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m,padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mConv2d(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU()\n","\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'MaxUnpool2D'"],"ename":"AttributeError","evalue":"module 'torch.nn' has no attribute 'MaxUnpool2D'","output_type":"error"}]}]}