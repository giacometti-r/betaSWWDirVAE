{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Txx9lE4wfb4D",
        "outputId": "8d02eea6-40c2-4311-e36a-29b9661690c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HyperNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(HyperNetwork, self).__init__()\n",
        "        # Define layers, output_size should match main network's weight size\n",
        "        self.fc1 = nn.Linear(input_size, 128)\n",
        "        self.fc2 = nn.Linear(128, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class MainNetwork(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(MainNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 128, bias=False)\n",
        "        self.fc2 = nn.Linear(128, output_size, bias=False)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        with torch.no_grad():\n",
        "            self.fc1.weight.fill_(1.0)\n",
        "            self.fc2.weight.fill_(1.0)\n",
        "            # Make weights non-trainable\n",
        "            self.fc1.weight.requires_grad = False\n",
        "            self.fc2.weight.requires_grad = False\n",
        "\n",
        "    def forward(self, x, weights):\n",
        "        w1_elements = self.fc1.in_features * self.fc1.out_features\n",
        "        w2_elements = self.fc2.in_features * self.fc2.out_features\n",
        "\n",
        "        # Reshape weights according to the dimensions of the layer's weight matrix\n",
        "        w1 = weights[:w1_elements].reshape(self.fc1.out_features, self.fc1.in_features)\n",
        "        w2 = weights[w1_elements:w1_elements + w2_elements].reshape(self.fc2.out_features, self.fc2.in_features)\n",
        "\n",
        "        x = F.linear(x, weight=w1)\n",
        "        x = F.relu(x)\n",
        "        x = F.linear(x, weight=w2)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "input_size = 100\n",
        "output_size = 10\n",
        "\n",
        "# Initialize MainNetwork\n",
        "main_network = MainNetwork(input_size, output_size)\n",
        "\n",
        "# Calculate the correct output size for HyperNetwork\n",
        "hypernetwork_output_size = (input_size * 128) + (128 * output_size)\n",
        "\n",
        "# Initialize HyperNetwork with the corrected output size\n",
        "hypernetwork = HyperNetwork(input_size, hypernetwork_output_size)\n",
        "\n",
        "# Example input\n",
        "x = torch.randn(1, input_size)\n",
        "\n",
        "# Forward pass\n",
        "hypernetwork_weights = hypernetwork(x)\n",
        "hypernetwork_weights = hypernetwork_weights.squeeze()  # Remove the extra batch dimension\n",
        "output = main_network(x, hypernetwork_weights)\n",
        "\n",
        "# Check if the output is correct\n",
        "print(output.shape)\n"
      ]
    }
  ]
}