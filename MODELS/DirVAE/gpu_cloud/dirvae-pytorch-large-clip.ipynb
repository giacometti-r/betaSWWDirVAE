{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efd4c5fae90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    !pip3 install matplotlib\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from typing import Union, Iterable, List, Dict, Tuple, Optional, cast\n",
    "from torch.utils._foreach_utils import _group_tensors_by_device_and_dtype\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = torch.rand(size=(100,50))\n",
    "\n",
    "# dirichlet = torch.distributions.Dirichlet(z)\n",
    "# p_set = dirichlet.sample()\n",
    "# N, K = p_set.size()\n",
    "\n",
    "# mu1_tilde = torch.mean(p_set, axis=0)\n",
    "# mu2_tilde = torch.mean(torch.pow(p_set,2), axis=0)\n",
    "\n",
    "# S = 1/K * torch.sum((mu1_tilde-mu2_tilde) / (mu2_tilde-torch.pow(mu1_tilde,2)))\n",
    "\n",
    "# alpha = S/N * torch.sum(p_set, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.dense1 = nn.Linear(784, 500)\n",
    "        self.dense2 = nn.Linear(500, 500)\n",
    "        self.dense3 = nn.Linear(500, latent_dim)\n",
    "\n",
    "    def sample(self, alpha_hat):\n",
    "        u = torch.rand(size=alpha_hat.size(), requires_grad=False).to(device)\n",
    "        v = torch.pow(u * alpha_hat * torch.exp(torch.lgamma(alpha_hat)),1.0/alpha_hat)\n",
    "        z = v / torch.sum(v)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        alpha_hat = x.view(-1, 28*28)\n",
    "        alpha_hat = F.relu(self.dense1(alpha_hat))\n",
    "        alpha_hat = F.relu(self.dense2(alpha_hat))\n",
    "        alpha_hat = F.softplus(self.dense3(alpha_hat))\n",
    "        z = self.sample(alpha_hat)\n",
    "        return z, alpha_hat\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dense1 = nn.Linear(latent_dim, 500)\n",
    "        self.dense2 = nn.Linear(500, 28*28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = F.relu(self.dense1(x))\n",
    "        return nn.Sigmoid()(self.dense2(x_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(DirVAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z, alpha_hat = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, alpha_hat, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(x_hat, x, alpha_hat, alpha, epsilon=5e-16):\n",
    "    \n",
    "    #likelihood = F.binary_cross_entropy_with_logits(x_hat, x.view(-1, 28*28), reduction='sum') #remove output func\n",
    "    #likelihood = torch.sum(torch.pow(x_hat - x.view(-1, 28*28), 2))\n",
    "    likelihood = torch.abs(torch.sum(x.view(-1, 28*28) * torch.log(x_hat + epsilon) + (1.0 - x.view(-1, 28*28)) * torch.log(1.0-x_hat + epsilon)))\n",
    "    \n",
    "    lgamma_alpha = torch.lgamma(alpha).to(device)\n",
    "    lgamma_alpha_hat = torch.lgamma(alpha_hat).to(device)\n",
    "    digamma_alpha_hat = torch.digamma(alpha_hat).to(device)\n",
    "    \n",
    "    kld = torch.sum(lgamma_alpha - lgamma_alpha_hat + (alpha_hat - alpha) * digamma_alpha_hat)\n",
    "    \n",
    "#     if torch.isnan(likelihood):\n",
    "#         print('LIKELIHOOD IS NAN')\n",
    "        \n",
    "#     if torch.isnan(kld):\n",
    "#         print('KLD IS NAN') \n",
    "\n",
    "    return likelihood + kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_alpha_mme(z):\n",
    "    dirichlet = torch.distributions.Dirichlet(z)\n",
    "    p_set = dirichlet.sample()\n",
    "    N, K = p_set.size()\n",
    "\n",
    "    mu1_tilde = torch.mean(p_set, axis=0)\n",
    "    mu2_tilde = torch.mean(torch.pow(p_set,2), axis=0)\n",
    "\n",
    "    S = 1/K * torch.sum((mu1_tilde-mu2_tilde) / (mu2_tilde-torch.pow(mu1_tilde,2)))\n",
    "\n",
    "    alpha = S/N * torch.sum(p_set, axis=0)\n",
    "    \n",
    "    return alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__del__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__name__', '__ne__', '__new__', '__next__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'close', 'gi_code', 'gi_frame', 'gi_running', 'gi_yieldfrom', 'send', 'throw']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m temp3 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mdense3\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mgrad\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mdir\u001b[39m(params))\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m())\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m([torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(temp1)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m     45\u001b[0m        torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(temp2)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m     46\u001b[0m        torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(temp3)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()])\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss at end of epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "                                           batch_size=100, shuffle=True, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "                                          batch_size=100, shuffle=True, collate_fn=lambda x: tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "# nll_loss = nn.NLLLoss()\n",
    "\n",
    "latent_dim = 50\n",
    "\n",
    "model = DirVAE(latent_dim).to(device)\n",
    "\n",
    "params = model.parameters()\n",
    "optimizer = optim.Adam(params, lr=5e-4)\n",
    "\n",
    "alpha =  ((1 - 1/latent_dim) * torch.ones(size=(latent_dim,))).to(device)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (x, _) in enumerate(train_loader): \n",
    "        #x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            x_hat, alpha_hat, z = model(x)\n",
    "            loss = ELBO(x_hat, x, alpha_hat, alpha)\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, 750.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "     # = [p.grad for p in if p.grad is not None]\n",
    "    temp1 = model.encoder.dense1.weight.grad\n",
    "    temp2 = model.encoder.dense2.weight.grad\n",
    "    temp3 = model.encoder.dense3.weight.grad\n",
    "    print(dir(params))\n",
    "    print(params.next())\n",
    "    \n",
    "    print([torch.linalg.vector_norm(temp1).cpu().numpy().tolist(),\n",
    "           torch.linalg.vector_norm(temp2).cpu().numpy().tolist(),\n",
    "           torch.linalg.vector_norm(temp3).cpu().numpy().tolist()])\n",
    "    print(f'loss at end of epoch {epoch}: {loss.item()}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (val_x, _) in enumerate(test_loader):\n",
    "            val_x = val_x.to(device)\n",
    "            val_x_hat, val_alpha_hat, val_z = model(val_x)\n",
    "            test_loss = ELBO(val_x_hat, val_x, val_alpha_hat, alpha)\n",
    "    print(f'test loss at end of epoch {epoch}: {test_loss.item()}')\n",
    "    \n",
    "    if epoch == 0:\n",
    "        print('ORIGINAL')\n",
    "        plt.imshow(test_loader.dataset[0][0].numpy().reshape(28,28))\n",
    "        plt.show()\n",
    "    with torch.no_grad():\n",
    "        sample = test_loader.dataset[0][0].to(device)\n",
    "        img, img_alpha_hat, img_z = model(sample)\n",
    "    img = torch.sigmoid(img)\n",
    "    img = img.to('cpu').numpy().reshape(28,28)\n",
    "    print('RECONSTRUCTED')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "#     if epoch % 50 == 0 and epoch >= 200 and epoch < 299:\n",
    "#         alpha = update_alpha_mme(z)\n",
    "#         print('alpha:', alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
