{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0mnQwRA32LT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import Loss\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_h7JDBY2FSGK"
      },
      "outputs": [],
      "source": [
        "import psutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ob-VvZSYQZv"
      },
      "source": [
        "#### This script is just to ensure whether training process will be distributed across multiple GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQ9N6HkT60UZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True) # Currently, memory growth needs to be the same across GPUs\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLTgttJ45n3m"
      },
      "outputs": [],
      "source": [
        "tfd = tfp.distributions\n",
        "# run_opts = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom = True)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gbRDzhU5g7t"
      },
      "outputs": [],
      "source": [
        "class Encoder(Model):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        initializer = tf.keras.initializers.GlorotNormal(seed=42)\n",
        "        self.dense1 = Dense(500, activation='relu', kernel_initializer=initializer)\n",
        "        self.dense2 = Dense(500, activation='relu', kernel_initializer=initializer)\n",
        "        self.dense3 = Dense(128, activation='relu', kernel_initializer=initializer)\n",
        "        self.dense4 = Dense(latent_dim, activation='softplus', kernel_initializer=initializer)\n",
        "\n",
        "    def sample(self, alpha_hat, alpha, beta):\n",
        "        shape = (alpha_hat.get_shape().as_list()[-2],alpha_hat.get_shape().as_list()[-1])\n",
        "        u = tf.random.uniform(shape=shape, minval=0, maxval=1)\n",
        "        v = tf.math.multiply(u,alpha)\n",
        "        v = tf.math.multiply(v, tf.math.exp(tf.math.lgamma(alpha)))\n",
        "        v = tf.math.pow(v, tf.math.divide(1.0,alpha))\n",
        "        v = tf.math.divide(v,beta)\n",
        "        z = tf.math.divide(v,tf.math.reduce_sum(v)) #normalise\n",
        "        #z ,_= tf.linalg.normalize(v)\n",
        "\n",
        "        return z\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0]\n",
        "        alpha = inputs[1]\n",
        "        beta = inputs[2]\n",
        "        #x = tf.reshape(x, (-1,28*28))\n",
        "        alpha_hat = self.dense1(x)\n",
        "        alpha_hat = self.dense2(alpha_hat)\n",
        "        #alpha_hat = self.dense3(alpha_hat)\n",
        "        alpha_hat = self.dense4(alpha_hat)\n",
        "        z = self.sample(alpha_hat, alpha, beta)\n",
        "        return z, alpha_hat\n",
        "\n",
        "class Decoder(Model):\n",
        "    def __init__(self, original_shape):\n",
        "        super(Decoder, self).__init__()\n",
        "        initializer = tf.keras.initializers.GlorotNormal(seed=42)\n",
        "        self.dense1 = Dense(500, activation='relu',kernel_initializer=initializer)\n",
        "        self.dense2 = Dense(256, activation='relu',kernel_initializer=initializer)\n",
        "        self.dense3 = Dense(512, activation='relu',kernel_initializer=initializer)\n",
        "        self.dense4 = Dense(original_shape, activation='sigmoid',kernel_initializer=initializer)\n",
        "\n",
        "    def call(self, x):\n",
        "        x_hat = self.dense1(x)\n",
        "        #x_hat = self.dense2(x_hat)\n",
        "        #x_hat = self.dense3(x_hat)\n",
        "        x_hat = self.dense4(x_hat)\n",
        "        #x_hat = tf.reshape(x_hat,[-1,28,28])\n",
        "        return x_hat\n",
        "\n",
        "class DirVAE(Model):\n",
        "    def __init__(self, latent_dim, original_dim):\n",
        "        super(DirVAE, self).__init__()\n",
        "        self.encoder = Encoder(latent_dim)\n",
        "        self.decoder = Decoder(original_dim)\n",
        "\n",
        "    def compile(self, optimizer, loss):\n",
        "        super().compile(optimizer)\n",
        "        self.loss = loss\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z, alpha_hat = self.encoder(inputs)\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat, alpha_hat\n",
        "\n",
        "@tf.function\n",
        "def ELBO(log_likelihood_loss, y_pred, y_true, alpha, alpha_hat):\n",
        "    ll_loss = log_likelihood_loss(y_true, y_pred)\n",
        "\n",
        "    kld_loss = tf.math.subtract(tf.math.lgamma(alpha), tf.math.lgamma(alpha_hat))\n",
        "    kld_loss = tf.math.add(kld_loss, tf.math.multiply(tf.math.subtract(alpha_hat, alpha),tf.math.digamma(alpha_hat)))\n",
        "    kld_loss = tf.reduce_sum(kld_loss)\n",
        "\n",
        "    # kld_loss = tf.reduce_sum(\n",
        "    #     tf.math.lgamma(alpha) -\n",
        "    #     tf.math.lgamma(alpha_hat) +\n",
        "    #     (alpha_hat - alpha) * tf.math.digamma(alpha_hat)\n",
        "    # )\n",
        "\n",
        "    return (tf.math.add(ll_loss, tf.math.maximum(0.0,kld_loss))), ll_loss, kld_loss\n",
        "\n",
        "@tf.function\n",
        "def update_alpha_mme(alpha, samples=50):\n",
        "\n",
        "    dirichlet = tfd.Dirichlet(alpha)\n",
        "    p_set = dirichlet.sample([samples])\n",
        "    N, K = p_set.shape\n",
        "\n",
        "    mu1_tilde = tf.math.reduce_mean(p_set, axis=0)\n",
        "    mu2_tilde = tf.math.reduce_mean(tf.math.pow(p_set,2), axis=0)\n",
        "\n",
        "    S = tf.math.reduce_mean(tf.math.divide((tf.math.subtract(mu1_tilde, mu2_tilde)), (tf.math.subtract(mu2_tilde, tf.math.pow(mu1_tilde,2)))), axis=0)\n",
        "\n",
        "    alpha = tf.math.multiply(tf.math.divide(S,N), tf.math.reduce_sum(p_set, axis=0))\n",
        "\n",
        "    return alpha\n",
        "\n",
        "def ClipIfNotNone(grad):\n",
        "            if grad is None:\n",
        "                return grad\n",
        "            return tf.clip_by_value(grad, -1000, 1000)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train_reshaped = x_train.reshape((-1,28*28)).astype(\"float32\") / 255\n",
        "x_test_reshaped = x_test.reshape((-1,28*28)).astype(\"float32\") / 255\n",
        "\n",
        "print('GPU:', tf.config.list_physical_devices('GPU'))\n",
        "tf.config.run_functions_eagerly(False)\n",
        "\n",
        "latent_dim = 50\n",
        "original_dim = 28*28\n",
        "\n",
        "model = DirVAE(latent_dim, original_dim)\n",
        "\n",
        "log_likelihood_loss = tf.keras.losses.BinaryCrossentropy(reduction='sum')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
        "\n",
        "batch_size = 100\n",
        "alpha = (1.0-(1.0/latent_dim)) * tf.ones((latent_dim,))\n",
        "beta = 1.0 * tf.ones((latent_dim,))\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_reshaped))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_reshaped))\n",
        "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "lowest_loss = np.inf\n",
        "epochs = 1500\n",
        "\n",
        "# Gradient Accumulation Setup\n",
        "accumulation_steps = 8  # Number of steps to accumulate gradients\n",
        "accumulated_gradients = [tf.zeros_like(tv) for tv in model.trainable_variables]\n",
        "\n",
        "# Function to log RAM usage\n",
        "# def log_memory_usage():\n",
        "#     memory = psutil.virtual_memory()\n",
        "#     print(f\"RAM memory % used: {memory.percent}%\")\n",
        "#     print(f\"Total memory: {memory.total / (1024**3):.2f} GB\")\n",
        "#     print(f\"Available memory: {memory.available / (1024**3):.2f} GB\")\n",
        "#     print(f\"Used memory: {memory.used / (1024**3):.2f} GB\")\n",
        "\n",
        "# Training Loop with Gradient Accumulation and RAM Logging\n",
        "for epoch in range(epochs):\n",
        "    log_memory_usage()  # Log RAM usage at the start of each epoch\n",
        "    print('___________________________')\n",
        "    print(f'_____EPOCH_{epoch}________')\n",
        "    print('___________________________')\n",
        "    for step, x_batch_train in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            prediction, alpha_hat = model([x_batch_train, alpha, beta], training=True)\n",
        "            loss_value, ll_loss, kld_loss = ELBO(log_likelihood_loss, prediction, x_batch_train, alpha, alpha_hat)\n",
        "\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        accumulated_gradients = [(acum_grad + grad) for acum_grad, grad in zip(accumulated_gradients, grads)]\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_weights))\n",
        "            accumulated_gradients = [tf.zeros_like(tv) for tv in model.trainable_variables]\n",
        "\n",
        "        if step % 250 == 0:\n",
        "            print(f\"Training loss at step {step}: {float(loss_value):.4f}\")\n",
        "            print(f\"LL loss at step {step}: {float(ll_loss):.4f}\")\n",
        "            print(f\"kld loss at step {step}: {float(kld_loss):.4f}\\n\")\n",
        "\n",
        "    # log_memory_usage()  # Log RAM usage at the end of each epoch\n",
        "    # gc.collect()  # Run garbage collection to free up memory\n",
        "    # tf.keras.backend.clear_session()  # Clear the current TensorFlow session\n",
        "\n",
        "\n",
        "    val_loss = []\n",
        "    for step, (x_batch_test) in enumerate(test_dataset):\n",
        "        val_prediction, val_alpha_hat = model([x_batch_test, alpha, beta], training=False)\n",
        "        val_loss_value, val_ll_loss, val_kld_loss = ELBO(log_likelihood_loss, val_prediction, x_batch_test, alpha, val_alpha_hat)\n",
        "        val_loss.append(val_loss_value.numpy())\n",
        "    val_loss = np.mean(np.array(val_loss))\n",
        "    print('AVERAGE VALIDATION LOSS:', val_loss)\n",
        "    print()\n",
        "\n",
        "    ##UPDATE ALPHA\n",
        "    if epoch % 10 == 0 and epoch != 0:\n",
        "        alpha = update_alpha_mme(alpha)\n",
        "\n",
        "    if loss_value.numpy() < lowest_loss:\n",
        "        model.save_weights('my_model', overwrite=True, save_format='tf', options=None)\n",
        "        lowest_loss = loss_value.numpy()\n",
        "    print('Alpha:', alpha.numpy())\n",
        "\n",
        "print('FINAL ALPHA:', alpha.numpy())\n",
        "FINAL_ALPHA = alpha\n",
        "\n",
        "# # At the end of each epoch or at certain intervals\n",
        "# gc.collect()  # Run garbage collection to free up memory\n",
        "# tf.keras.backend.clear_session()  # Clear the current TensorFlow session\n",
        "\n",
        "# # After running your model, to log memory usage\n",
        "# !nvidia-smi  # If running in a notebook environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf99vERkBnnl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kL1pyjDNpBS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jcu9K6ncNDDm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUOr1oyrLxFU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "121v02FFMc6m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCmGSnggMc4P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Pdg10V4LxDS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VchARmFuJj1P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3j2Yw14Htgk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hQg3PH5HKJg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APcNhqmjGjKJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmRaBYviBnlc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_Y7BQN5BnjX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
