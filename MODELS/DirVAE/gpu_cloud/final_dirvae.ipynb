{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import Loss\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "        self.dense1 = Dense(500, activation='relu', kernel_initializer=initializer)\n",
    "        self.dropout1 = Dropout(0.5)\n",
    "        self.dense2 = Dense(500, activation='relu', kernel_initializer=initializer)\n",
    "        self.dropout2 = Dropout(0.5)\n",
    "        self.dense3 = Dense(128, activation='relu', kernel_initializer=initializer)\n",
    "        self.dropout3 = Dropout(0.5)\n",
    "        self.dense4 = Dense(latent_dim, activation='softplus', kernel_initializer=initializer)\n",
    "\n",
    "    def sample(self, alpha_hat, alpha, beta):\n",
    "        shape = (alpha_hat.get_shape().as_list()[-2],alpha_hat.get_shape().as_list()[-1])\n",
    "        u = tf.random.uniform(shape=shape, minval=0, maxval=1)\n",
    "        v = tf.math.multiply(u,alpha_hat)\n",
    "        v = tf.math.multiply(v, tf.math.exp(tf.math.lgamma(alpha_hat)))\n",
    "        v = tf.math.pow(v, tf.math.divide(1.0,alpha_hat))\n",
    "        v = tf.math.divide(v,beta)\n",
    "        z = tf.math.divide(v,tf.math.reduce_sum(v)) #sum to one\n",
    "\n",
    "        return z, v\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        alpha = inputs[1]\n",
    "        beta = inputs[2]\n",
    "        alpha_hat = self.dense1(x)\n",
    "        #alpha_hat = self.dropout1(alpha_hat)\n",
    "        alpha_hat = self.dense2(alpha_hat)\n",
    "        #alpha_hat = self.dropout2(alpha_hat)\n",
    "        #alpha_hat = self.dense3(alpha_hat)\n",
    "        #alpha_hat = self.dropout3(alpha_hat)\n",
    "        alpha_hat = self.dense4(alpha_hat)\n",
    "        z, v = self.sample(alpha_hat, alpha, beta)\n",
    "        return z, alpha_hat, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, original_shape):\n",
    "        super(Decoder, self).__init__()\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "        self.dense1 = Dense(500, activation='relu',kernel_initializer=initializer)\n",
    "        self.dropout1 = Dropout(0.5)\n",
    "        self.dense2 = Dense(256, activation='relu',kernel_initializer=initializer)\n",
    "        self.dropout2 = Dropout(0.5)\n",
    "        self.dense3 = Dense(512, activation='relu',kernel_initializer=initializer)\n",
    "        self.dropout3 = Dropout(0.5)\n",
    "        self.dense4 = Dense(original_shape, activation='sigmoid',kernel_initializer=initializer)\n",
    "\n",
    "    def call(self, x):\n",
    "        x_hat = self.dense1(x)\n",
    "        #x_hat = self.dropout1(x_hat)\n",
    "        #x_hat = self.dense2(x_hat)\n",
    "        #x_hat = self.dropout2(x_hat)\n",
    "        #x_hat = self.dense3(x_hat)\n",
    "        #x_hat = self.dropout3(x_hat)\n",
    "        x_hat = self.dense4(x_hat)\n",
    "        #x_hat = tf.reshape(x_hat,[-1,28,28])\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirVAE(Model):\n",
    "    def __init__(self, latent_dim, original_dim):\n",
    "        super(DirVAE, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(original_dim)\n",
    "\n",
    "    def compile(self, optimizer, loss):\n",
    "        super().compile(optimizer)\n",
    "        self.loss = loss\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z, alpha_hat, v = self.encoder(inputs)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z, alpha_hat, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def ELBO(log_likelihood_loss, y_pred, y_true, alpha, alpha_hat):\n",
    "    ll_loss = log_likelihood_loss(y_true, y_pred)\n",
    "    \n",
    "    beta = 1.0\n",
    "    kld_loss = tf.math.subtract(tf.math.lgamma(alpha), tf.math.lgamma(alpha_hat))\n",
    "    kld_loss = tf.math.add(kld_loss, tf.math.multiply(tf.math.subtract(alpha_hat, alpha),tf.math.digamma(alpha_hat)))\n",
    "    kld_loss = tf.reduce_sum(kld_loss)\n",
    "\n",
    "    return (tf.math.add(ll_loss, tf.math.multiply(beta,tf.math.maximum(0.0,kld_loss)))), ll_loss, kld_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def update_alpha_mme(z, samples=1, epsilon=1e-13):\n",
    "    \n",
    "    epsilon = tf.convert_to_tensor(epsilon)\n",
    "    dirichlet = tfd.Dirichlet(z)\n",
    "    p_set = dirichlet.sample([samples])\n",
    "    _, N, K = p_set.shape\n",
    "    p_set = tf.reshape(p_set, (100,50,))\n",
    "\n",
    "    mu1_tilde = tf.math.reduce_mean(p_set, axis=0)\n",
    "    mu2_tilde = tf.math.reduce_mean(tf.math.pow(p_set,2), axis=0)\n",
    "\n",
    "    S = tf.math.reduce_mean(tf.math.divide((tf.math.subtract(mu1_tilde, mu2_tilde)), (tf.math.subtract(mu2_tilde, tf.math.pow(mu1_tilde,2))) + epsilon), axis=0)\n",
    "\n",
    "    alpha = tf.math.multiply(tf.math.divide(S,N), tf.math.reduce_sum(p_set, axis=0)) + epsilon\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClipIfNotNone(grad):\n",
    "            if grad is None:\n",
    "                return grad\n",
    "            return tf.clip_by_value(grad, -10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train_reshaped = x_train.reshape((-1,28*28)).astype(\"float32\") / 255\n",
    "x_test_reshaped = x_test.reshape((-1,28*28)).astype(\"float32\") / 255\n",
    "\n",
    "print('GPU:', tf.config.list_physical_devices('GPU'))\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "latent_dim = 50\n",
    "original_dim = 28*28\n",
    "\n",
    "model = DirVAE(latent_dim, original_dim)\n",
    "\n",
    "log_likelihood_loss = tf.keras.losses.BinaryCrossentropy(reduction='sum', from_logits=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "batch_size = 100\n",
    "alpha = (1.0-(1.0/latent_dim)) * tf.ones((latent_dim,))\n",
    "beta = 1.0 * tf.ones((latent_dim,))\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_reshaped))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_reshaped))\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "lowest_loss = np.inf\n",
    "epochs = 300\n",
    "count = 0\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('___________________________')\n",
    "    print(f'_____EPOCH_{epoch}________')\n",
    "    print('___________________________')\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction, z, alpha_hat, v = model([x_batch_train, alpha, beta], training=True)\n",
    "            loss_value, ll_loss, kld_loss = ELBO(log_likelihood_loss, prediction, x_batch_train, alpha, alpha_hat)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        grads = [ClipIfNotNone(grad) for grad in grads]\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        if step % 250 == 0:\n",
    "            print(f\"Training loss at step {step}: {float(loss_value):.4f}\")\n",
    "            print(f\"LL loss at step {step}: {float(ll_loss):.4f}\")\n",
    "            print(f\"kld loss at step {step}: {float(kld_loss):.4f}\\n\")\n",
    "\n",
    "\n",
    "    val_loss = []\n",
    "    for step, (x_batch_test) in enumerate(test_dataset):\n",
    "        val_prediction, val_z, val_alpha_hat, val_v = model([x_batch_test, alpha, beta], training=False)\n",
    "        val_loss_value, val_ll_loss, val_kld_loss = ELBO(log_likelihood_loss, val_prediction, x_batch_test, alpha, val_alpha_hat)\n",
    "        val_loss.append(val_loss_value.numpy())\n",
    "    val_loss = np.mean(np.array(val_loss))\n",
    "    print('AVERAGE VALIDATION LOSS:', val_loss)\n",
    "    print()\n",
    "\n",
    "    ##UPDATE ALPHA\n",
    "    # if epoch % 20 == 0 and epoch != 0 and count <= 2:\n",
    "    #     alpha = update_alpha_mme(z)\n",
    "    #     print('Alpha:', alpha.numpy())\n",
    "    #     count += 1\n",
    "\n",
    "    train_loss.append(loss_value.numpy())\n",
    "    \n",
    "    inputs = [\n",
    "        tf.convert_to_tensor(x_train[0].reshape((1,28*28))),\n",
    "        alpha,\n",
    "        beta\n",
    "    ]\n",
    "\n",
    "    image, img_z, img_alpha_hat, img_v = model(inputs)\n",
    "    if epoch == 0:\n",
    "        plt.imshow(x_train[0])\n",
    "        plt.show()\n",
    "        #plt.savefig('/reconstructed_images/original.png')\n",
    "    \n",
    "    plt.imshow(image[0].numpy().reshape((28,28)))\n",
    "    plt.show()\n",
    "    #plt.savefig(f'/reconstructed_images/epoch_{epoch}.png')\n",
    "\n",
    "    if val_loss_value.numpy() < lowest_loss:\n",
    "        model.save_weights('/weights/my_model', overwrite=True, save_format='tf', options=None)\n",
    "        lowest_loss = val_loss_value.numpy()\n",
    "\n",
    "df_loss = pd.DataFrame(train_loss)\n",
    "df_loss.to_csv('loss.csv', index=False)\n",
    "print('FINAL ALPHA:', alpha.numpy())\n",
    "FINAL_ALPHA = alpha"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
